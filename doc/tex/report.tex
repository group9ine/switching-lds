\documentclass[10pt, twocolumn, headings=small, footlines=1, DIV=calc]{scrartcl}
\addtokomafont{disposition}{\normalfont\bfseries}
\usepackage{preamble}
\recalctypearea

\title{Bayesian Inference for\\Switching Linear Dynamical Systems}
\author{D. Bacilieri, L. Barbiero, G. Bordin, A. Pitteri}
\date{\today}

\begin{document}
\maketitle

\section{Model description}
A switching linear dynamical system -- also known as \emph{switching state space
model} -- is defined, borrowing the notation from \textcite{Linderman17}, by the
set of discrete-time stochastic equations
\begin{align}
  x_{t} &= A_{z_{t}} x_{t - 1} + b_{z_{t}} + v_{t}, \\
  y_{t} &= C_{z_{t}} x_{t} + d_{z_{t}} + w_{t},
\end{align}
where $v_{t} \in \R^{M}$ and $w_{t} \in \R^{N}$ are Gaussian-distributed
random vectors with mean zero and variance $Q_{z_{t}}$, $S_{z_{t}}$
respectively. The vectors $y_{t} \in \R^{N}$, $t = 1, \ldots, T$ may represent
a time series of observations, while the $x_{t} \in \R^{M}$ are a set of
continuous latent states linked together by linear dynamics defined by the
matrices $A_{k} \in \R^{M \times M}$ and bias vectors $b_{k} \in \R^{M}$. The
transformation between $x$ and $y$ is also linear, through the matrices $C_{k}
\in \R^{N \times M}$ and bias vectors $d_{k} \in \R^{N}$.

The linear parameters $A_{k}$, $b_{k}$, $C_{k}$, $d_{k}$ form a discrete set of
$K$ elements, and a discrete latent variable $z_{t} \in \{1, \ldots, K\}$ sets
the specific instances in use at time step $t$. The variable $z$ evolves over
time as a Markov process, meaning that $z_{t}$ is conditionally independent of
all previous states except for its immediate predecessor $z_{t-1}$:
\begin{equation}
  p(z_{t} \given z_{t-1}, z_{t-2}, \ldots, z_{1}) = p(z_{t} \given z_{t-1}).
\end{equation}
We will denote the probability to transition from $z_{t-1} = j$ to $z_{t} = k$
with $\pi_{jk}$. The transition $z_{t-1} \to z_{t}$ effectively modifies the
linear dynamics from $x_{t-1}$ to $x_{t}$ and the linear transformation from
$x_{t}$ to $y_{t}$, \emph{switching} from one regime to another.

Given a set of data points $y_{t}$, the goal is then to infer the posterior
distribution of the parameter set
\begin{equation}
  \theta = \{ \pi_{k}, A_{k}, b_{k}, Q_{k}, C_{k}, d_{k}, S_{k} \},
\end{equation}
where $x_{1:T}$ denotes the whole sequence $x_{1}, x_{2}, \ldots, x_{T}$, and
$\pi_{k}$ the \emph{k}th row of the transition matrix.

\section{Implementation in Stan}
To set up a Monte Carlo sampling scheme, we chose to work with the Stan
\autocite{Stan24} programming language and its implementation in R through the
package \texttt{rstan} \autocite{RStan24}. Sampling in Stan is done by default
using a variant of the Hamiltonian Monte Carlo scheme called \enquote{No-U-Turn
sampler} or \textsc{nuts} \autocite{Carpenter17}.

The model cannot be implemented directly as it is, in the sense of specifying a
categorical likelihood for the transition $z_{t} \to z_{t+1}$ and Gaussian
likelihoods for $x_{t} \given x_{t-1}$ and $y_{t} \given x_{t}$, because Stan
does not allow the definition of integer parameters: so, one should marginalize
over the hidden discrete states. Besides Stanâ€™s limitations in this regard, the
resulting strategy -- known in the literature as \emph{forward algorithm} -- is
more efficient than the straightforward implementation in sampling
low-probability states, and is commonly used in similar inference problems
involving hidden Markov models or other state space models
\autocite{Damiano2018}.

\subsection{The forward algorithm}
The basic idea behind the forward algorithm is to exploit a recursive
relationship to build the full likelihood: indeed, consider the quantity
\begin{equation}
  \Gamma_{t}(k) \coloneq p(z_{t} = k, x_{1:t}, y_{1:t}).
\end{equation}
By summing over the $z$ states at $t-1$ first and then using the chain rule
repeatedly, we can write
\begin{equation}
  \begin{split}
    \Gamma_{t}(k) &= \sum_{j=1}^{K}
      p(z_{t} = k, z_{t-1} = j, x_{1:t}, y_{1:t}) \\
      &= \!\begin{multlined}[t]
        p(y_{t} \given z_{t} = k, x_{t}) \,
        p(x_{t} \given z_{t} = k, x_{t-1}) \\
        \cdot \sum_{j=1}^{K} \pi_{jk}\,
          p(z_{t-1} = j, x_{1:t-1}, y_{1:t-1}),
      \end{multlined}
  \end{split}
\label{eq:gamma-relation}
\end{equation}
where we have recognized the conditional probability $p(z_{t} = k \given z_{t-1}
= j, x_{1:t-1}, y_{1:t-1}) = p(z_{t} = k \given z_{t-1} = j)$ as the element
$(j, k)$ of the transition matrix $\pi$. The first two terms outside the sum are
the likelihoods of $y_{t}$ and $x_{t}$, and because of the model definition they
only depend on $z_{t}$, $x_{t}$ and $x_{t-1}$. Also, they are simply Gaussian
densities:
\begin{align}
  \lkl_{k}(y_{t}) &\coloneq p(y_{t} \given z_{t} = k, x_{t})
    = \nrm(C_{k} x_{t} + d_{k}, S_{k}), \\
  \lkl_{k}(x_{t}) &\coloneq p(x_{t} \given z_{t} = k, x_{t-1})
    = \nrm(A_{k} x_{t-1} + b_{k}, Q_{k}).
\end{align}
Then, the remaining terms in the sum in~\eqref{eq:gamma-relation} are nothing
else than $\Gamma_{t-1}(j)$, giving us the recursive relation we needed:
\begin{equation}
  \Gamma_{t}(k) = \lkl_{k}(y_{t})\, \lkl_{k}(x_{t})
    \sum_{j=1}^{K} \pi_{jk} \Gamma_{t-1}(j).
\end{equation}

Indeed, to retrieve the full joint likelihood of the sequences $x_{1:T}$ and
$y_{1:T}$ we only need to marginalize the $\Gamma$ at the last time step $T$
over the discrete states $k = 1, 2, \ldots, K$:
\begin{equation}
  p(x_{1:T}, y_{1:T}) = \sum_{k=1}^{K} p(z_{t} = k, x_{1:T}, y_{1:T})
    = \sum_{k=1}^{K} \Gamma_{T} (k).
\end{equation}
To recursively build $\Gamma_{t}$ up to time $T$ we need
$\mathcal{O}(TK^{2})$ operations, because of the double marginalization over
$z_{t}$ and $z_{t-1}$. To initialize the recursion,
\begin{equation}
  \begin{split}
    \Gamma_{1}(k) &= p(z_{1} = k, x_{1}, y_{1}) \\
                  &= \lkl_{1}(y_{1}) \,
                    p(x_{1} \given z_{1} = k) \, p(z_{1} = k).
  \end{split}
\end{equation}
The last two terms are the prior distributions on $x_{1}$ and $z_{1}$. We chose
a multivariate Gaussian for the first and a uniform distribution over the $K$
states for the second. 

At this point, we also need the prior distributions for the dynamical
parameters. Following the suggestion from \textcite{Linderman17}, we chose
matrix-normal-inverse-Wishart priors:
\begin{align}
  (A_{k}, b_{k}), Q_{k} &\sim \MNIW(M_{x}, \Omega_{x}, \Psi_{x}, \nu_{x}) \\
  (C_{k}, d_{k}), S_{k} &\sim \MNIW(M_{y}, \Omega_{y}, \Psi_{y}, \nu_{y}).
\end{align}
Here $M_{x} \in \R^{M \times M + 1}$ and $M_{y} \in \R^{N \times (M + 1)}$ are
the mean matrices of the matrix normals, $\Omega_x, \Omega_y \in \R^{(M + 1)
\times (M + 1)}$ their between-column covariance matrices, while $\Psi_x \in
\R^{M \times M}$ and $\Psi_{y} \in \R^{N \times N}$ are the scale matrices of
the inverse Wisharts and $\nu_{x}, \nu_{y}$ their degrees of freedom. The
returned random matrices with $M + 1$ columns are then split between the
matrices $A_{k}$ and $C_{k}$ and their corresponding bias vectors $b_{k}$ and
$d_{k}$.

\printbibliography
\end{document}
