---
title: Switching Linear Dynamical Systems
author: Davide Bacilieri, Lorenzo Barbiero, Guglielmo Bordin, Alessio Pitteri
date: "`r gsub('^0', '', format(Sys.Date(), '%d %B %Y'))`"
lang: en-GB
output:
  tufte::tufte_html:
    css: style.css
    toc: true
bibliography: refs.bib
link-citations: yes
---

<script type="text/x-mathjax-config"> 
  MathJax.Hub.Config({ displayAlign: "left", displayIndent: "2em" })
</script>

\renewcommand{\theta}{\vartheta}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\RRi}[1]{\RR^{#1}}
\newcommand{\RRii}[2]{\RR^{#1 \times #2}}
\newcommand{\iidsim}{\overset{\mathrm{iid}}{\sim}}
\newcommand{\norm}[2]{\mathcal{N}(#1, #2)}
\newcommand{\Dir}{\operatorname{Dir}}
\newcommand{\MNIW}{\operatorname{MNIW}}
\newcommand{\pisb}{\pi_{\mathrm{SB}}}
\newcommand{\given}[1][]{\;#1\vert\;}
\newcommand{\II}{\mathbb{I}}
\newcommand{\PG}{\operatorname{PG}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\tran}{^{\mkern-1.5mu\mathsf{T}}}

```{r, echo = FALSE}
# load R packages and set up
library(rstan)
options(mc.cores = parallel::detectCores() - 2)
rstan_options(auto_write = TRUE)
```
# Base model without recursion
Let’s start from the basic SLDS (Switching Linear Dynamical Systems) model –
*basic* in the sense that we will not include the dependence of the current
latent state from the past observation: latent states $z_t$ will only depend
on $z_{t - 1}$ [@Linderman17].

The goal is to infer a kind of hidden-state Markov model on a time series
$\{y_t\}_{t = 1}^{T}$. We build a set of $T$ latent states $z_t \in \{1, 2,
\dots K\}$. $z_t$ will define the linear dynamics followed by the system at time
$t$, so a transition in the latent space will correspond to a change in the
linear dynamics in the observable space.

The dynamics in the latent space is Markovian:

$$
  z_{t + 1} \sim \pi_{z_t},
$$

where $\pi_k$ denotes the $k$-th row of the transition matrix (following the
notation from Linderman et al. 2017). The discrete state $z_t$ determines the
linear dynamical system used at time $t$ to set the evolution of a *continuous*
latent state $x_t \in \RRi{M}$:

$$
  x_{t + 1} = A_{z_{t + 1}} x_{t} + b_{z_{t + 1}} + v_t, \quad v_t \iidsim
  \norm{0}{Q_{z_{t + 1}}},
$$

where $A_k, Q_k \in \RRii{M}{M}$ and $b_k \in \RRi{M}$. Note that there are $K$
possible values for these three parameters, $K$ being the number of $z$ states.

The final passage is to go back to the visible space, transforming the latent
state $x_t$ into the corresponding observation $y_t \in \RRi{N}$:

$$
  y_t = C_{z_t} x_t + d_{z_t} + w_t, \quad w_t \iidsim \norm{0}{S_{z_t}},
$$

where $C_k \in \RRii{N}{M}$, $S_k \in \RRii{N}{N}$ and $d_k \in \RRi{N}$. Once
again, $z_t$ sets the linear transformation by selecting among the $K$ possible
values of these three parameters.

Putting everything together, the model’s parameter set is

$$
  \theta = \{(\pi_k, A_k, Q_k, b_k, C_k, S_k, d_k)\}_{k = 1}^K.
$$

Regarding the priors, Linderman et al. choose a Dirichlet on the rows of the
transition matrix, and matrix normal inverse Wishart priors on the other
parameters:

\begin{align}
          \pi_k &\iidsim \Dir(\alpha), \\
  A_k, b_k, Q_k &\iidsim \MNIW(\lambda), \\
  C_k, d_k, S_k &\iidsim \MNIW(\eta).
\end{align}

## Setting up the Stan model

```{r, echo = FALSE, comment = ""}
cat(paste(readLines("stan/slds.stan"), collapse = "\n"))
```

# Adding recursion: rSLDS
Linderman et al. propose enriching the model by adding recursion, manifested as
a conditional dependence of $z_{t + 1}$ on the preceding continuous latent state
$x_t$. Instead of a standard multiclass logistic regression with softmax, the
conditional distribution is expressed through a stick-breaking function $\pisb
\colon \RRi{K - 1} \to [0, 1]^K$:

$$
  z_{t + 1} \sim \pisb(\nu_{t + 1}), \quad \nu_{t + 1} = R_{z_t} x_t + r_{z_t},
$$

where $R_k \in \RRii{K-1}{M}$ and $r_k \in \RRi{K-1}$. Therefore, instead of the
simpler dependence rule $z_{t + 1} \sim \pi_{z_t}$, we have this more involved
generating process for the discrete latent states. Notice that the Markov
dependence of $z_{t + 1}$ on $z_t$ is still there, through the linear
reweighting of $x_t$ with bias performed by $R_k$ and $r_k$.

The stick-breaking distribution is defined as

\begin{align}
        \pisb(\nu) &= (\pisb^{(1)}(\nu), \dots, \pisb^{(K)}(\nu)), \\
  \pisb^{(k)}(\nu) &= \sigma(\nu_k)
                      \prod_{j < k}^{\vphantom{K}} (1 - \sigma(\nu_j)), \\
  \pisb^{(K)}(\nu) &= \prod_{j = 1}^{K - 1} (1 - \sigma(\nu_j)),
\end{align}

where $\sigma(x)$ denotes the logistic function $e^x / (1 + e^x)$. You can
visualize the process as follows: the first element takes a portion
$\sigma(\nu_1)$ of the unit ‘stick’, the second element takes a portion
$\sigma(\nu_2)$ of the remaining stick, which now has a length of $1 -
\sigma(\nu_1)$, and so on. Hence, the name ‘stick-breaking’. Using the
expression of $\sigma(x)$ we can also write

\begin{align}
  \pisb^{(k)}(\nu) &= \prod_{j \leq k} \frac{e^{\nu_k}}{1 + e^{\nu_j}}, \\
  \pisb^{(K)}(\nu) &= \prod_{j = 1}^{K - 1} \frac{1}{1 + e^{\nu_j}}.
\end{align}

Borrowing the notation from Linderman et al. once again, we write the likelihood
$p(z \given x)$ as

$$
  p(z_{t + 1} \given x_t) = \prod_{k = 1}^{K - 1}
  \frac{(e^{\nu_{t+1, k}})^{\II[z_{t + 1} = k]}}{(1 + e^{\nu_{t+1,
  k}})^{\II[z_{t + 1} \geq k]}}.
$$

with $\II[\cdot]$ denoting the indicator function. Remember that $\nu_{t + 1}$
is a function of $x_t$.

Now, if we were to use this likelihood with a Gaussian prior on $x$, we would
get a pretty messy non-Gaussian posterior $p(x \given z)$. A possible solution
[@Linderman15] is given by the Pólya-gamma augmentation scheme, first described
in @Polson13. The idea is to introduce $K$ auxiliary variables $\omega =
\{\omega_k\}_{k = 1}^{K}$ and exploit the identity

$$
  \frac{e^{a\nu}}{(1 + e^\nu)^b} = \frac{e^{\kappa\nu}}{2^b}
  \int_0^\infty e^{-\omega \nu^2/2} p(\omega) \, d\omega,
  \quad \omega \sim \PG(b, 0).
$$

where $\PG(b, c)$ denotes the Pólya-gamma probability density. We can recognize
the terms in the product of $p(z_{t + 1} \given x_t)$ as the expression in the
left-hand side, with $a = \II[z_{t + 1} = k]$ and $b = \II[z_{t + 1} \geq k]$.
Then, if we condition also on $\omega_t$ the integral goes away, leaving

$$
  p(z_{t + 1} \given x_t, \omega_t) \propto \prod_{k = 1}^{K - 1}
  \exp\biggl(\kappa_{t+1, k} \nu_{t+1, k} - \frac{1}{2} \omega_{t, k}
  \nu_{t+1, k}^2\biggr).
$$

Here we can complete the square in the exponent:

$$
  \kappa \nu - \frac{\omega \nu^2}{2} = - \frac{\omega}{2}\biggl(\nu -
  \frac{\kappa}{\omega}\biggl)^{\!2} + \frac{\kappa^2}{2 \omega}.
$$

The last term is constant with respect to $\nu_{t + 1}$ – it only depends on
$\omega_t$ and $z_{t + 1}$ – so we can leave it out of the product:

$$
  p(z_{t + 1} \given x_t, \omega_t) \propto \prod_{k = 1}^{K - 1}
  \exp\biggl[-\frac{\omega_{t, k}}{2}\biggl(\nu_{t+1, k} - \frac{\kappa_{t+1,
  k}}{\omega_{t,k}}\biggr)^{\!2}\biggr].
$$


Finally, we can write this in vectorial form by defining the matrix $\Omega_t =
\diag(\omega_t)$, and recognize a multinomial Gaussian distribution in $\nu_{t +
1}$:

$$
\begin{split}
  p(z_{t + 1} \given x_t, \omega_t) &\propto \exp\biggl[-\frac{1}{2}(\nu_{t+1} -
  \Omega_t^{-1} \kappa_{t + 1})\tran \Omega_t (\nu_{t + 1} - \Omega_t^{-1}
  \kappa_{t+1})\biggr] \\
  &\propto \norm{\nu_{t + 1} \given \Omega_t^{-1} \kappa_{t+1}}{\Omega_t^{-1}}.
\end{split}
$$

Also, let’s consider the integrand of the $e^{a\nu}/(1 + e^\nu)^b$ identity as a
conditional density of $\omega \given \nu$,

$$
  p(\omega \given b, \nu) \propto e^{-\omega\nu^2/2} p_{\mathrm{PG}}(\omega
  \given b, 0).
$$

From Posson, Scott, and Windle (2013) we know that a random variable $\omega \sim
\PG(b, c)$ has precisely this probability density, identifying $c$ with $\nu$.
Therefore, we can add the following update rule for the Pólya-gamma auxiliary
variables:

$$
  \omega \sim \PG(\II[z_{t+1} \geq k], \nu_{t+1, k}).
$$
